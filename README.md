# Home tasks from the course "Natural language processing" in MIPT

- [Word2vec](https://github.com/Jesaig/nlp-course-mipt/tree/main/bert-classification)  
Implementation of the word2vec algorithm ([cs224n assigment](https://web.stanford.edu/class/cs224n/assignments/a2.pdf))
- [Language models](https://github.com/Jesaig/nlp-course-mipt/tree/main/language-models)  
[YSDA NLP assignment](https://github.com/yandexdataschool/nlp_course/tree/2020/week03_lm)
- [Seq2seq with attention](https://github.com/Jesaig/nlp-course-mipt/tree/main/seq2seq-attention)  
Implementation of the seq2seq model with multiplicative attention ([cs224n assignment](https://web.stanford.edu/class/cs224n/assignments/a4.pdf))
- [Text classification with BERT](https://github.com/Jesaig/nlp-course-mipt/tree/main/word2vec)  
I use pretraned BERT for generating word embeddings and then train the classifier.
